from __future__ import annotations
import sys
import abc
import ast
import contextlib
import re
from typing import List, Tuple, Dict
import tiktoken


# from metaagent.information import Message


def get_class(class_name):
    # ğŸ‘‡ï¸ returns None if class with given name doesn't exist
    return getattr(sys.modules[__name__], class_name, None)


class Singleton(abc.ABCMeta, type):
    """
    Singleton metaclass for ensuring only one instance of a class.
    """

    _instances = {}

    def __call__(cls, *args, **kwargs):
        """Call method for the singleton metaclass."""
        if cls not in cls._instances:
            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
        return cls._instances[cls]


class OutputParser:

    @classmethod
    def parse_blocks(cls, text: str):
        # é¦–å…ˆæ ¹æ®"##"å°†æ–‡æœ¬åˆ†å‰²æˆä¸åŒçš„block
        blocks = text.split("##")

        # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨æ¯ä¸ªblockçš„æ ‡é¢˜å’Œå†…å®¹
        block_dict = {}

        # éå†æ‰€æœ‰çš„block
        for block in blocks:
            # å¦‚æœblockä¸ä¸ºç©ºï¼Œåˆ™ç»§ç»­å¤„ç†
            if block.strip() != "":
                # å°†blockçš„æ ‡é¢˜å’Œå†…å®¹åˆ†å¼€ï¼Œå¹¶åˆ†åˆ«å»æ‰å‰åçš„ç©ºç™½å­—ç¬¦
                block_title, block_content = block.split("\n", 1)
                # LLMå¯èƒ½å‡ºé”™ï¼Œåœ¨è¿™é‡Œåšä¸€ä¸‹ä¿®æ­£
                if block_title[-1] == ":":
                    block_title = block_title[:-1]
                block_dict[block_title.strip()] = block_content.strip()

        return block_dict

    @classmethod
    def parse_code(cls, text: str, lang: str = "") -> str:
        pattern = rf'```{lang}.*?\s+(.*?)```'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            code = match.group(1)
        else:
            raise Exception
        return code

    @classmethod
    def parse_str(cls, text: str):
        text = text.split("=")[-1]
        text = text.strip().strip("'").strip("\"")
        return text

    @classmethod
    def parse_file_list(cls, text: str) -> list[str]:
        # Regular expression pattern to find the tasks list.
        pattern = r'\s*(.*=.*)?(\[.*\])'

        # Extract tasks list string using regex.
        match = re.search(pattern, text, re.DOTALL)
        if match:
            tasks_list_str = match.group(2)

            # Convert string representation of list to a Python list using ast.literal_eval.
            tasks = ast.literal_eval(tasks_list_str)
        else:
            tasks = text.split("\n")
        return tasks
    
    @staticmethod
    def parse_python_code(text: str) -> str:
        for pattern in (
            r'(.*?```python.*?\s+)?(?P<code>.*)(```.*?)', 
            r'(.*?```python.*?\s+)?(?P<code>.*)', 
        ):
            match = re.search(pattern, text, re.DOTALL)
            if not match:
                continue
            code = match.group("code")
            if not code:
                continue
            with contextlib.suppress(Exception):
                ast.parse(code)
                return code
        raise ValueError("Invalid python code")

    @classmethod
    def parse_data(cls, data):
        block_dict = cls.parse_blocks(data)
        parsed_data = {}
        for block, content in block_dict.items():
            # å°è¯•å»é™¤codeæ ‡è®°
            try:
                content = cls.parse_code(text=content)
            except Exception:
                pass

            # å°è¯•è§£ælist
            try:
                content = cls.parse_file_list(text=content)
            except Exception:
                pass
            parsed_data[block] = content
        return parsed_data

    @classmethod
    def parse_data_with_mapping(cls, data, mapping):
        block_dict = cls.parse_blocks(data)
        parsed_data = {}
        for block, content in block_dict.items():
            # å°è¯•å»é™¤codeæ ‡è®°
            try:
                content = cls.parse_code(text=content)
            except Exception:
                pass
            typing_define = mapping.get(block, None)
            if isinstance(typing_define, tuple):
                typing = typing_define[0]
            else:
                typing = typing_define
            if typing == List[str] or typing == List[Tuple[str, str]]:
                # å°è¯•è§£ælist
                try:
                    content = cls.parse_file_list(text=content)
                except Exception:
                    pass
            # TODO: å¤šä½™çš„å¼•å·å»é™¤æœ‰é£é™©ï¼ŒåæœŸå†è§£å†³
            # elif typing == str:
            #     # å°è¯•å»é™¤å¤šä½™çš„å¼•å·
            #     try:
            #         content = cls.parse_str(text=content)
            #     except Exception:
            #         pass
            parsed_data[block] = content
        return parsed_data




TOKEN_COSTS = {
    "gpt-3.5-turbo": {"prompt": 0.0015, "completion": 0.002},
    "gpt-3.5-turbo-0301": {"prompt": 0.0015, "completion": 0.002},
    "gpt-3.5-turbo-0613": {"prompt": 0.0015, "completion": 0.002},
    "gpt-3.5-turbo-16k": {"prompt": 0.003, "completion": 0.004},
    "gpt-3.5-turbo-16k-0613": {"prompt": 0.003, "completion": 0.004},
    "gpt-4-0314": {"prompt": 0.03, "completion": 0.06},
    "gpt-4": {"prompt": 0.03, "completion": 0.06},
    "gpt-4-32k": {"prompt": 0.06, "completion": 0.12},
    "gpt-4-32k-0314": {"prompt": 0.06, "completion": 0.12},
    "gpt-4-0613": {"prompt": 0.06, "completion": 0.12},
    "text-embedding-ada-002": {"prompt": 0.0004, "completion": 0.0},
}


TOKEN_MAX = {
    "gpt-3.5-turbo": 4096,
    "gpt-3.5-turbo-0301": 4096,
    "gpt-3.5-turbo-0613": 4096,
    "gpt-3.5-turbo-16k": 16384,
    "gpt-3.5-turbo-16k-0613": 16384,
    "gpt-4-0314": 8192,
    "gpt-4": 8192,
    "gpt-4-32k": 32768,
    "gpt-4-32k-0314": 32768,
    "gpt-4-0613": 8192,
    "text-embedding-ada-002": 8192,
}


def count_message_tokens(messages, model="gpt-3.5-turbo-0613"):
    """Return the number of tokens used by a list of messages."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        print("Warning: model not found. Using cl100k_base encoding.")
        encoding = tiktoken.get_encoding("cl100k_base")
    if model in {
        "gpt-3.5-turbo-0613",
        "gpt-3.5-turbo-16k-0613",
        "gpt-4-0314",
        "gpt-4-32k-0314",
        "gpt-4-0613",
        "gpt-4-32k-0613",
    }:
        tokens_per_message = 3
        tokens_per_name = 1
    elif model == "gpt-3.5-turbo-0301":
        tokens_per_message = 4  # every message follows <|start|>{role/name}\n{content}<|end|>\n
        tokens_per_name = -1  # if there's a name, the role is omitted
    elif "gpt-3.5-turbo" in model:
        print("Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.")
        return count_message_tokens(messages, model="gpt-3.5-turbo-0613")
    elif "gpt-4" in model:
        print("Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.")
        return count_message_tokens(messages, model="gpt-4-0613")
    else:
        raise NotImplementedError(
            f"""num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens."""
        )
    num_tokens = 0
    for message in messages:
        num_tokens += tokens_per_message
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == "name":
                num_tokens += tokens_per_name
    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
    return num_tokens


def count_string_tokens(string: str, model_name: str) -> int:
    """
    Returns the number of tokens in a text string.

    Args:
        string (str): The text string.
        model_name (str): The name of the encoding to use. (e.g., "gpt-3.5-turbo")

    Returns:
        int: The number of tokens in the text string.
    """
    encoding = tiktoken.encoding_for_model(model_name)
    return len(encoding.encode(string))


def get_max_completion_tokens(messages: list[dict], model: str, default: int) -> int:
    """Calculate the maximum number of completion tokens for a given model and list of messages.

    Args:
        messages: A list of messages.
        model: The model name.

    Returns:
        The maximum number of completion tokens.
    """
    if model not in TOKEN_MAX:
        return default
    return TOKEN_MAX[model] - count_message_tokens(messages) - 1


# def actionoutout_schema_to_mapping(schema: Dict) -> Dict:
#     """
#     directly traverse the `properties` in the first level.
#     schema structure likes
#     ```
#     {
#         "title":"prd",
#         "type":"object",
#         "properties":{
#             "Original Requirements":{
#                 "title":"Original Requirements",
#                 "type":"string"
#             },
#         },
#         "required":[
#             "Original Requirements",
#         ]
#     }
#     ```
#     """
#     mapping = dict()
#     for field, property in schema["properties"].items():
#         if property["type"] == "string":
#             mapping[field] = (str, ...)
#         elif property["type"] == "array" and property["items"]["type"] == "string":
#             mapping[field] = (List[str], ...)
#         elif property["type"] == "array" and property["items"]["type"] == "array":
#             # here only consider the `Tuple[str, str]` situation
#             mapping[field] = (List[Tuple[str, str]], ...)
#     return mapping


# def serialize_message(message: Message):
#     message_cp = copy.deepcopy(message)  # avoid `instruct_content` value update by reference
#     ic = message_cp.instruct_content
#     if ic:
#         # model create by pydantic create_model like `pydantic.main.prd`, can't pickle.dump directly
#         schema = ic.schema()
#         mapping = actionoutout_schema_to_mapping(schema)

#         message_cp.instruct_content = {"class": schema["title"], "mapping": mapping, "value": ic.dict()}
#     msg_ser = pickle.dumps(message_cp)

#     return msg_ser


# def deserialize_message(message_ser: str) -> Message:
#     message = pickle.loads(message_ser)
#     if message.instruct_content:
#         ic = message.instruct_content
#         ic_obj = ActionOutput.create_model_class(class_name=ic["class"], mapping=ic["mapping"])
#         ic_new = ic_obj(**ic["value"])
#         message.instruct_content = ic_new

#     return message